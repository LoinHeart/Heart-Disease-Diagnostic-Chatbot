{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataset for training:\n",
    "#### Step-by-step guide:\n",
    "#### Load the dataset.\n",
    "\n",
    "#### Inspect the data to understand its structure and features.\n",
    "\n",
    "#### Check for missing values and handle them if necessary.\n",
    "\n",
    "#### Encode categorical features if any.\n",
    "\n",
    "#### Normalize or scale numerical features (optional but recommended for logistic regression).\n",
    "\n",
    "#### Split the data into training and testing sets.\n",
    "\n",
    "#### Ready for Logistic Regression Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let me start with loading and inspecting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "frame = pd.read_csv('heart_disease_uci.csv')\n",
    "frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has 920 entries and 16 columns. Here's a breakdown of the columns:\n",
    "\n",
    "id: Identifier for each patient.\n",
    "\n",
    "age: Age of the patient.\n",
    "\n",
    "sex: Gender of the patient (Male/Female).\n",
    "\n",
    "dataset: Source of the dataset (this might not be necessary for prediction).\n",
    "\n",
    "cp: Chest pain type (categorical).\n",
    "\n",
    "trestbps: Resting blood pressure (some missing values).\n",
    "\n",
    "chol: Cholesterol level (some missing values).\n",
    "\n",
    "fbs: Fasting blood sugar (categorical, some missing values).\n",
    "\n",
    "restecg: Resting electrocardiographic results (categorical).\n",
    "\n",
    "thalch: Maximum heart rate achieved (some missing values).\n",
    "\n",
    "exang: Exercise induced angina (categorical, some missing values).\n",
    "\n",
    "oldpeak: ST depression induced by exercise (some missing values).\n",
    "\n",
    "slope: Slope of the peak exercise ST segment (categorical, many missing values).\n",
    "\n",
    "ca: Number of major vessels (0-3) colored by fluoroscopy (many missing values).\n",
    "\n",
    "thal: Thalassemia (categorical, many missing values).\n",
    "\n",
    "num: Diagnosis of heart disease (target variable: 0 = no disease, 1+ = disease)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps:\n",
    "\n",
    "1.Handle missing values.\n",
    "\n",
    "2.Convert categorical variables into numeric formats.\n",
    "\n",
    "3.Drop irrelevant columns like id and possibly dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop irrelevant columns (id and dataset)\n",
    "df = frame.drop(columns=['id', 'dataset'])\n",
    "\n",
    "# Check the number of missing values in each column\n",
    "missing_values = df.isnull().sum()\n",
    "\n",
    "missing_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle missing values:\n",
    "\n",
    "You can fill or drop missing values depending on your strategy:\n",
    "\n",
    "Drop rows with many missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df.dropna(subset=['trestbps', 'chol', 'thalch', 'oldpeak', 'ca','thal','fbs','restecg','exang','slope'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill missing values with mean/median:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values for numeric columns with the mean= Model Accuracy: 0.----- (or another strategy as needed)\n",
    "# Fill missing values using .loc to avoid view issues\n",
    "df_cleaned.loc[:, 'trestbps'] = df_cleaned['trestbps'].fillna(df_cleaned['trestbps'].mean())\n",
    "df_cleaned.loc[:, 'chol'] = df_cleaned['chol'].fillna(df_cleaned['chol'].mean())\n",
    "df_cleaned.loc[:, 'thalch'] = df_cleaned['thalch'].fillna(df_cleaned['thalch'].mean())\n",
    "df_cleaned.loc[:, 'oldpeak'] = df_cleaned['oldpeak'].fillna(df_cleaned['oldpeak'].mean())\n",
    "df_cleaned.loc[:, 'ca'] = df_cleaned['ca'].fillna(df_cleaned['ca'].mean())\n",
    "\n",
    "\"\"\"# Fill missing values for numeric columns with the median= Model Accuracy: 0.------ (or another strategy as needed)\n",
    "df_cleaned['trestbps'].fillna(df_cleaned['trestbps'].median(), inplace=True)\n",
    "df_cleaned['chol'].fillna(df_cleaned['chol'].median(), inplace=True)\n",
    "df_cleaned['thalch'].fillna(df_cleaned['thalch'].median(), inplace=True)\n",
    "df_cleaned['oldpeak'].fillna(df_cleaned['oldpeak'].median(), inplace=True)\n",
    "df_cleaned['ca'].fillna(df_cleaned['ca'].median(), inplace=True)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values using .loc\n",
    "df_cleaned.loc[:, 'slope'] = df_cleaned['slope'].fillna(df_cleaned['slope'].mode()[0])\n",
    "df_cleaned.loc[:, 'thal'] = df_cleaned['thal'].fillna(df_cleaned['thal'].mode()[0])\n",
    "df_cleaned.loc[:, 'restecg'] = df_cleaned['restecg'].fillna(df_cleaned['restecg'].mode()[0])\n",
    "df_cleaned.loc[:, 'exang'] = df_cleaned['exang'].fillna(df_cleaned['exang'].mode()[0])\n",
    "df_cleaned.loc[:, 'fbs'] = df_cleaned['fbs'].fillna(df_cleaned['fbs'].mode()[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Label Encoder for encode categories column to numaric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Assuming 'df' is your DataFrame and 'cp' is your column with categories\n",
    "label_encoder = LabelEncoder()\n",
    "df_cleaned['cp'] = label_encoder.fit_transform(df_cleaned['cp'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert categorical variables:\n",
    "\n",
    "Label encoding converts each category into a unique integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "categorical_cols = df_cleaned.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Apply Label Encoding to each categorical column\n",
    "for col in categorical_cols:\n",
    "    df_cleaned[col] = label_encoder.fit_transform(df_cleaned[col])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numeric columns\n",
    "numeric_columns = df_cleaned.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Calculate IQR for numeric columns only\n",
    "Q1 = df_cleaned[numeric_columns].quantile(0.25)\n",
    "Q3 = df_cleaned[numeric_columns].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Filter out outliers from the DataFrame for numeric columns only\n",
    "df_cleaned = df_cleaned[~((df_cleaned[numeric_columns] < (Q1 - 1.5 * IQR)) | (df_cleaned[numeric_columns] > (Q3 + 1.5 * IQR))).any(axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the num column contains values ranging from 0 to 4, we can convert it into two classes:\n",
    "\n",
    "0 for no disease\n",
    "\n",
    "1 for any heart disease (i.e., values 1, 2, 3, or 4).\n",
    "\n",
    "Steps to convert num to a binary variable:\n",
    "\n",
    "Convert num column to binary:\n",
    "\n",
    "If num == 0, it represents no heart disease.\n",
    "\n",
    "If num > 0, it represents the presence of heart disease (convert to 1).\n",
    "\n",
    "Here's the code to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'num' to a binary variable\n",
    "df_cleaned.loc[:, 'num'] = df_cleaned['num'].apply(lambda x: 0 if x == 0 else 1)\n",
    "\n",
    "\n",
    "# Check the distribution of the binary target variable\n",
    "print(df_cleaned['num'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into features and target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_cleaned.drop(columns=['num'])\n",
    "y = df_cleaned['num']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale numerical features (optional for Logistic Regression):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into train and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ready for Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Train Logistic Regression Model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model on the training set\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 2: Make predictions and evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Step 3: Save the trained model to a file\n",
    "model_filename = 'logistic_regression_heart_disease.pkl'\n",
    "joblib.dump(model, model_filename)\n",
    "\n",
    "print(f\"Model saved as {model_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase two:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deeplearning model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "# Standardize data (DNNs typically benefit from standardizing inputs)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# Example with L2 regularization and Dropout\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(1024, activation='relu', input_shape=(X_train.shape[1],), \n",
    "                          kernel_regularizer=regularizers.l2(0.001)),  # L2 regularization\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.4),  # Dropout to prevent overfitting\n",
    "    tf.keras.layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(16, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(8, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(4, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(2, activation='relu'),\n",
    "    \n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "optimizer = tf.keras.optimizers.AdamW(learning_rate=0.001)\n",
    "  # Reduced learning rate\n",
    "# Compile the model\n",
    "model.compile(optimizer='AdamW', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50,  batch_size=4, \n",
    "                    callbacks=[early_stopping,reduce_lr])\n",
    "\n",
    "#print(model.summary())\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"Test_Accuracy_84.52%.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the Scaler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "# Save scaler\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "\n",
    "\"\"\"# Load scaler\n",
    "scaler = joblib.load('scaler.pkl')\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Classifier, Which is ML Algorithim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train a Gradient Boosting Classifier (XGBoost)\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"XGBoost Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBClassifier, ML Alo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = XGBClassifier()\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 500],\n",
    "    'learning_rate': [0.001, 0.1],\n",
    "    'max_depth': [8, 5, 4],\n",
    "}\n",
    "grid_search = GridSearchCV(model, param_grid, cv=8, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best Score: {grid_search.best_score_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create  Flask Server with Model API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from flask import Flask, request, jsonify\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load the saved Keras model and Scaler\n",
    "model = tf.keras.models.load_model('Test_Accuracy_84.52%.keras')\n",
    "\n",
    "# Load the scaler (ensure it's in the same directory)\n",
    "with open('scaler.pkl', 'rb') as f:\n",
    "    scaler = pickle.load(f)\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    # Get JSON data from the POST request\n",
    "    data = request.json\n",
    "    features = np.array([[\n",
    "        data['age'], data['sex'], data['cp'], data['trestbps'], data['chol'], \n",
    "        data['fbs'], data['restecg'], data['thalach'], data['exang'], \n",
    "        data['oldpeak'], data['slope'], data['ca'], data['thal']\n",
    "    ]])\n",
    "\n",
    "    # Scale the input data\n",
    "    scaled_features = scaler.transform(features)\n",
    "\n",
    "    # Make a prediction\n",
    "    prediction = model.predict(scaled_features)\n",
    "    predicted_class = np.argmax(prediction, axis=1)[0]\n",
    "\n",
    "    # Return the result as JSON\n",
    "    return jsonify({\n",
    "        'predicted_class': int(predicted_class),\n",
    "        'message': 'You are unlikely to have heart disease.' if predicted_class == 0 else 'You are likely to have heart disease.'\n",
    "    })\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streamlit Frontend Interacting with the Flask API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Function to gather user input\n",
    "def get_user_input():\n",
    "    age = st.number_input('Age', min_value=0, max_value=120, value=30)\n",
    "    sex = st.selectbox('Sex (1 = Male, 0 = Female)', [0, 1])\n",
    "    cp = st.selectbox('Chest Pain Type (0-3)', [0, 1, 2, 3])\n",
    "    trestbps = st.number_input('Resting Blood Pressure', min_value=80, max_value=200, value=120)\n",
    "    chol = st.number_input('Cholesterol Level', min_value=100, max_value=600, value=200)\n",
    "    fbs = st.selectbox('Fasting Blood Sugar > 120 mg/dl (1 = True, 0 = False)', [0, 1])\n",
    "    restecg = st.selectbox('Resting Electrocardiographic Results (0-2)', [0, 1, 2])\n",
    "    thalach = st.number_input('Maximum Heart Rate Achieved', min_value=60, max_value=220, value=150)\n",
    "    exang = st.selectbox('Exercise Induced Angina (1 = Yes, 0 = No)', [0, 1])\n",
    "    oldpeak = st.number_input('ST Depression Induced by Exercise', min_value=0.0, max_value=6.0, value=1.0, step=0.1)\n",
    "    slope = st.selectbox('Slope of Peak Exercise ST Segment (0-2)', [0, 1, 2])\n",
    "    ca = st.number_input('Number of Major Vessels (0-4)', min_value=0, max_value=4, value=0)\n",
    "    thal = st.selectbox('Thalassemia (0 = Normal, 1 = Fixed Defect, 2 = Reversible Defect)', [0, 1, 2])\n",
    "\n",
    "    # Create a dictionary with the user input\n",
    "    user_data = {\n",
    "        'age': age, 'sex': sex, 'cp': cp, 'trestbps': trestbps, 'chol': chol,\n",
    "        'fbs': fbs, 'restecg': restecg, 'thalach': thalach, 'exang': exang,\n",
    "        'oldpeak': oldpeak, 'slope': slope, 'ca': ca, 'thal': thal\n",
    "    }\n",
    "\n",
    "    return user_data\n",
    "\n",
    "# Main function for Streamlit app\n",
    "def main():\n",
    "    st.title(\"Heart Disease Diagnostic Chat\")\n",
    "\n",
    "    # Gather input data from user\n",
    "    user_input = get_user_input()\n",
    "\n",
    "    # When user clicks 'Predict' button, send data to Flask API\n",
    "    if st.button(\"Predict\"):\n",
    "        # Send POST request to Flask API\n",
    "        url = 'http://127.0.0.1:5000/predict'  # Flask API URL\n",
    "        headers = {'Content-Type': 'application/json'}\n",
    "        response = requests.post(url, data=json.dumps(user_input), headers=headers)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            prediction = response.json()\n",
    "            st.write(f\"Prediction: {prediction['message']}\")\n",
    "        else:\n",
    "            st.write(\"Error: Could not connect to Flask API.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
